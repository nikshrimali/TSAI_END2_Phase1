{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0fbeaffcba064d35a6d29d75e61155a23ece1c55ca7e7176fe096951437ca84b8",
   "display_name": "Python 3.8.5 64-bit ('nlp_py': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentence_index                                             phrase  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "2               3                     Effective but too-tepid biopic   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "   splitset_label  \n",
       "0               1  \n",
       "1               1  \n",
       "2               2  \n",
       "3               2  \n",
       "4               2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_index</th>\n      <th>phrase</th>\n      <th>splitset_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>The Rock is destined to be the 21st Century 's...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>The gorgeously elaborate continuation of `` Th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Effective but too-tepid biopic</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>If you sometimes like to go to the movies to h...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Emerges as something rare , an issue movie tha...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_csv(\"data/stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\t\")\n",
    "\n",
    "dataset_split = pd.read_csv(\"data/stanfordSentimentTreebank/datasetSplit.txt\")\n",
    "split_data = sentences.merge(dataset_split, on=\"sentence_index\", how=\"left\")\n",
    "\n",
    "split_data.columns = [\"sentence_index\", \"phrase\", \"splitset_label\"]\n",
    "\n",
    "split_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          phrase  phrase ids  sentiment values overall sentiment  \\\n",
       "0            ! '       22935           0.52778           neutral   \n",
       "1           ! ''       18235           0.50000           neutral   \n",
       "2         ! Alas      179257           0.44444           neutral   \n",
       "3    ! Brilliant       22936           0.86111     very positive   \n",
       "4  ! Brilliant !       40532           0.93056     very positive   \n",
       "\n",
       "  sentiment_value  \n",
       "0               2  \n",
       "1               2  \n",
       "2               2  \n",
       "3               4  \n",
       "4               4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>phrase ids</th>\n      <th>sentiment values</th>\n      <th>overall sentiment</th>\n      <th>sentiment_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>! '</td>\n      <td>22935</td>\n      <td>0.52778</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>! ''</td>\n      <td>18235</td>\n      <td>0.50000</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>! Alas</td>\n      <td>179257</td>\n      <td>0.44444</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>! Brilliant</td>\n      <td>22936</td>\n      <td>0.86111</td>\n      <td>very positive</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>! Brilliant !</td>\n      <td>40532</td>\n      <td>0.93056</td>\n      <td>very positive</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "dictionary = pd.read_csv(\"data/stanfordSentimentTreebank/dictionary.txt\", sep=\"|\")\n",
    "dictionary.columns = [\"phrase\", \"phrase ids\"]\n",
    "# dictionary = dictionary.set_index(\"phrase ids\")\n",
    "\n",
    "sentiment_labels = pd.read_csv(\"data/stanfordSentimentTreebank/sentiment_labels.txt\", sep=\"|\")\n",
    "\n",
    "sentiment_labels[\"overall sentiment\"] = pd.cut(sentiment_labels[\"sentiment values\"], [0, 0.2, 0.4, 0.6, 0.8, 1.0], include_lowest=True ,labels=[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"])\n",
    "\n",
    "sentiment_labels[\"sentiment_value\"] = pd.cut(sentiment_labels[\"sentiment values\"], [0, 0.2, 0.4, 0.6, 0.8, 1.0], include_lowest=True ,labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "phrase_sentiments = dictionary.merge(sentiment_labels, on=\"phrase ids\")\n",
    "\n",
    "phrase_sentiments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               phrase  phrase ids  \\\n",
       "0   , The Sum of All Fears is simply a well-made a...      102340   \n",
       "1                        , `` They 're out there ! ''      221244   \n",
       "2   , is a temporal inquiry that shoulders its phi...      221388   \n",
       "3        - I also wanted a little alien as a friend !      221714   \n",
       "4   - West Coast rap wars , this modern mob music ...      221716   \n",
       "13              -LRB- A -RRB- rare , beautiful film .       13691   \n",
       "14  -LRB- Drumline -RRB- is entertaining for what ...       13695   \n",
       "15  -LRB- Schweiger is -RRB- talented and terribly...       13696   \n",
       "16  -LRB- Wendigo is -RRB- why we go to the cinema...       13697   \n",
       "17  ... Blade II is more enjoyable than the origin...       24114   \n",
       "19  ... Brian De Palma is utterly mad : cinema mad...      221765   \n",
       "20  ... Designed to provide a mix of smiles and te...      221766   \n",
       "27  ... Mafia , rap stars and hood rats butt their...       24121   \n",
       "41  ... a boring parade of talking heads and techn...      142768   \n",
       "51  ... a fun little timewaster , helped especiall...       43848   \n",
       "\n",
       "    sentiment values overall sentiment sentiment_value  sentence_index  \\\n",
       "0            0.88889     very positive               4            4860   \n",
       "1            0.61111          positive               3            7251   \n",
       "2            0.69444          positive               3            5477   \n",
       "3            0.69444          positive               3            5576   \n",
       "4            0.76389          positive               3            2338   \n",
       "13           0.95833     very positive               4           11623   \n",
       "14           0.73611          positive               3             224   \n",
       "15           0.77778          positive               3             145   \n",
       "16           0.65278          positive               3              14   \n",
       "17           0.91667     very positive               4             961   \n",
       "19           0.58333           neutral               2            1550   \n",
       "20           0.22222          negative               1            7900   \n",
       "27           0.65278          positive               3            1262   \n",
       "41           0.27778          negative               1            7763   \n",
       "51           0.65278          positive               3            1491   \n",
       "\n",
       "    splitset_label  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "13               2  \n",
       "14               2  \n",
       "15               2  \n",
       "16               2  \n",
       "17               2  \n",
       "19               3  \n",
       "20               3  \n",
       "27               3  \n",
       "41               3  \n",
       "51               3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>phrase ids</th>\n      <th>sentiment values</th>\n      <th>overall sentiment</th>\n      <th>sentiment_value</th>\n      <th>sentence_index</th>\n      <th>splitset_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>, The Sum of All Fears is simply a well-made a...</td>\n      <td>102340</td>\n      <td>0.88889</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>4860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>, `` They 're out there ! ''</td>\n      <td>221244</td>\n      <td>0.61111</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>7251</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>, is a temporal inquiry that shoulders its phi...</td>\n      <td>221388</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5477</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>- I also wanted a little alien as a friend !</td>\n      <td>221714</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5576</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>- West Coast rap wars , this modern mob music ...</td>\n      <td>221716</td>\n      <td>0.76389</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>2338</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-LRB- A -RRB- rare , beautiful film .</td>\n      <td>13691</td>\n      <td>0.95833</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>11623</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-LRB- Drumline -RRB- is entertaining for what ...</td>\n      <td>13695</td>\n      <td>0.73611</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>224</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-LRB- Schweiger is -RRB- talented and terribly...</td>\n      <td>13696</td>\n      <td>0.77778</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>145</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-LRB- Wendigo is -RRB- why we go to the cinema...</td>\n      <td>13697</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>... Blade II is more enjoyable than the origin...</td>\n      <td>24114</td>\n      <td>0.91667</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>961</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>... Brian De Palma is utterly mad : cinema mad...</td>\n      <td>221765</td>\n      <td>0.58333</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>1550</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>... Designed to provide a mix of smiles and te...</td>\n      <td>221766</td>\n      <td>0.22222</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7900</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>... Mafia , rap stars and hood rats butt their...</td>\n      <td>24121</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1262</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>... a boring parade of talking heads and techn...</td>\n      <td>142768</td>\n      <td>0.27778</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7763</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>... a fun little timewaster , helped especiall...</td>\n      <td>43848</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1491</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "main_dataframe = phrase_sentiments.merge(split_data, on=\"phrase\")\n",
    "main_dataframe[\"splitset_label\"] = main_dataframe[\"splitset_label\"].fillna(1).astype(int)\n",
    "main_dataframe = main_dataframe[main_dataframe.phrase.map(len)>4]\n",
    "\n",
    "main_dataframe = main_dataframe.groupby(\"splitset_label\")\n",
    "\n",
    "main_dataframe.head(5)"
   ]
  },
  {
   "source": [
    "from utils.augumentations import back_translation\n",
    "\n",
    "\n",
    "train_dataframe = main_dataframe.get_group(1)\n",
    "train_dataframe = train_dataframe.set_index(pd.Index(range(train_dataframe.shape[0])))\n",
    "# train_dataframe = train_dataframe.append(\n",
    "#     train_dataframe.apply(\n",
    "#         lambda row: {\n",
    "#             'phrase': back_translation(row['phrase']), 'phrase ids':  row['phrase ids'], \n",
    "#             'sentiment values': row['sentiment values'], 'overall sentiment': row['overall sentiment'], \n",
    "#             'sentiment_value': row['sentiment_value'], 'sentence_index': row['sentence_index'], \n",
    "#             'splitset_label': row['splitset_label']\n",
    "#         }, \n",
    "#         axis=1)[0],\n",
    "    # ignore_index = True)\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               phrase  phrase ids  \\\n",
       "0   , The Sum of All Fears is simply a well-made a...      102340   \n",
       "1                        , `` They 're out there ! ''      221244   \n",
       "2   , is a temporal inquiry that shoulders its phi...      221388   \n",
       "3        - I also wanted a little alien as a friend !      221714   \n",
       "4   - West Coast rap wars , this modern mob music ...      221716   \n",
       "5                   - greaseballs mob action-comedy .      142685   \n",
       "6   - spy action flick with Antonio Banderas and L...      221720   \n",
       "7   - style cross-country adventure ... it has spo...      221722   \n",
       "8                     -- but certainly hard to hate .      221739   \n",
       "9   -- but it makes for one of the most purely enj...      221741   \n",
       "10  -- is a crime that should be punishable by cha...      221750   \n",
       "11  -- that the ` true story ' by which All the Qu...      221754   \n",
       "12        -- was a fad that had long since vanished .      221761   \n",
       "13  ... Blade II is still top-heavy with blazing g...      181282   \n",
       "14  ... Despite lagging near the finish line , the...       62545   \n",
       "15  ... Ice Age treads predictably along familiar ...      103179   \n",
       "16  ... Jones , despite a definitely distinctive s...      103180   \n",
       "17  ... Liotta is put in an impossible spot becaus...      221768   \n",
       "18  ... Myers has turned his franchise into the mo...      103182   \n",
       "19  ... Pray does n't have a passion for the mater...      221770   \n",
       "\n",
       "    sentiment values overall sentiment sentiment_value  sentence_index  \\\n",
       "0            0.88889     very positive               4            4860   \n",
       "1            0.61111          positive               3            7251   \n",
       "2            0.69444          positive               3            5477   \n",
       "3            0.69444          positive               3            5576   \n",
       "4            0.76389          positive               3            2338   \n",
       "5            0.36111          negative               1            7166   \n",
       "6            0.16667     very negative               0           11305   \n",
       "7            0.70833          positive               3           11409   \n",
       "8            0.61111          positive               3            7163   \n",
       "9            0.81944     very positive               4            2732   \n",
       "10           0.27778          negative               1            6580   \n",
       "11           0.22222          negative               1            7778   \n",
       "12           0.41667           neutral               2           10865   \n",
       "13           0.16667     very negative               0           11495   \n",
       "14           0.77778          positive               3            3019   \n",
       "15           0.43056           neutral               2            5877   \n",
       "16           0.55556           neutral               2            5998   \n",
       "17           0.27778          negative               1            9890   \n",
       "18           0.50000           neutral               2            4879   \n",
       "19           0.30556          negative               1            2950   \n",
       "\n",
       "    splitset_label  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "5                1  \n",
       "6                1  \n",
       "7                1  \n",
       "8                1  \n",
       "9                1  \n",
       "10               1  \n",
       "11               1  \n",
       "12               1  \n",
       "13               1  \n",
       "14               1  \n",
       "15               1  \n",
       "16               1  \n",
       "17               1  \n",
       "18               1  \n",
       "19               1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>phrase ids</th>\n      <th>sentiment values</th>\n      <th>overall sentiment</th>\n      <th>sentiment_value</th>\n      <th>sentence_index</th>\n      <th>splitset_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>, The Sum of All Fears is simply a well-made a...</td>\n      <td>102340</td>\n      <td>0.88889</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>4860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>, `` They 're out there ! ''</td>\n      <td>221244</td>\n      <td>0.61111</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>7251</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>, is a temporal inquiry that shoulders its phi...</td>\n      <td>221388</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5477</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>- I also wanted a little alien as a friend !</td>\n      <td>221714</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5576</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>- West Coast rap wars , this modern mob music ...</td>\n      <td>221716</td>\n      <td>0.76389</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>2338</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>- greaseballs mob action-comedy .</td>\n      <td>142685</td>\n      <td>0.36111</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>- spy action flick with Antonio Banderas and L...</td>\n      <td>221720</td>\n      <td>0.16667</td>\n      <td>very negative</td>\n      <td>0</td>\n      <td>11305</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>- style cross-country adventure ... it has spo...</td>\n      <td>221722</td>\n      <td>0.70833</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>11409</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-- but certainly hard to hate .</td>\n      <td>221739</td>\n      <td>0.61111</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>7163</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-- but it makes for one of the most purely enj...</td>\n      <td>221741</td>\n      <td>0.81944</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>2732</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-- is a crime that should be punishable by cha...</td>\n      <td>221750</td>\n      <td>0.27778</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>6580</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-- that the ` true story ' by which All the Qu...</td>\n      <td>221754</td>\n      <td>0.22222</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7778</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-- was a fad that had long since vanished .</td>\n      <td>221761</td>\n      <td>0.41667</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>... Blade II is still top-heavy with blazing g...</td>\n      <td>181282</td>\n      <td>0.16667</td>\n      <td>very negative</td>\n      <td>0</td>\n      <td>11495</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>... Despite lagging near the finish line , the...</td>\n      <td>62545</td>\n      <td>0.77778</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>3019</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>... Ice Age treads predictably along familiar ...</td>\n      <td>103179</td>\n      <td>0.43056</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>5877</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>... Jones , despite a definitely distinctive s...</td>\n      <td>103180</td>\n      <td>0.55556</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>5998</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>... Liotta is put in an impossible spot becaus...</td>\n      <td>221768</td>\n      <td>0.27778</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>9890</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>... Myers has turned his franchise into the mo...</td>\n      <td>103182</td>\n      <td>0.50000</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>4879</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>... Pray does n't have a passion for the mater...</td>\n      <td>221770</td>\n      <td>0.30556</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>2950</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "test_dataframe = main_dataframe.get_group(2)\n",
    "test_dataframe = test_dataframe.set_index(pd.Index(range(test_dataframe.shape[0])))\n",
    "\n",
    "validaion_dataframe = main_dataframe.get_group(3)\n",
    "validaion_dataframe = validaion_dataframe.set_index(pd.Index(range(validaion_dataframe.shape[0])))\n",
    "\n",
    "train_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cb8271c390>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Import Library\n",
    "import random\n",
    "import torch, torchtext\n",
    "from torchtext.legacy import data\n",
    "\n",
    "# Manual Seed\n",
    "SEED = 43\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReviewComment = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
    "Rating = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('review', ReviewComment),('rating', Rating)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset([data.Example.fromlist([train_dataframe.phrase[i],train_dataframe.sentiment_value[i]], fields) for i in range(train_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.Dataset([data.Example.fromlist([test_dataframe.phrase[i],test_dataframe.sentiment_value[i]], fields) for i in range(test_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'review': ['...',\n",
       "  'a',\n",
       "  'haunting',\n",
       "  'vision',\n",
       "  ',',\n",
       "  'with',\n",
       "  'images',\n",
       "  'that',\n",
       "  'seem',\n",
       "  'more',\n",
       "  'like',\n",
       "  'disturbing',\n",
       "  'hallucinations',\n",
       "  '.'],\n",
       " 'rating': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "vars(test_dataset.examples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = data.Dataset([data.Example.fromlist([validaion_dataframe.phrase[i],validaion_dataframe.sentiment_value[i]], fields) for i in range(validaion_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReviewComment.build_vocab(train_dataset)\n",
    "Rating.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of input vocab :  16523\nSize of label vocab :  5\nTop 10 words appreared repeatedly : [('.', 7633), (',', 6706), ('the', 5714), ('of', 4170), ('and', 4152), ('a', 4147), ('to', 2844), ('-', 2566), ('is', 2403), (\"'s\", 2353)]\nLabels :  defaultdict(None, {3: 0, 1: 1, 2: 2, 4: 3, 0: 4})\n"
     ]
    }
   ],
   "source": [
    "print('Size of input vocab : ', len(ReviewComment.vocab))\n",
    "print('Size of label vocab : ', len(Rating.vocab))\n",
    "print('Top 10 words appreared repeatedly :', list(ReviewComment.vocab.freqs.most_common(10)))\n",
    "print('Labels : ', Rating.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = data.BucketIterator.splits((train_dataset, test_dataset), batch_size = 32, \n",
    "                                                            sort_key = lambda x: len(x.review),\n",
    "                                                            sort_within_batch=True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "with open('tokenizer.pkl', 'wb') as tokens: \n",
    "    pickle.dump(ReviewComment.vocab.stoi, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    # Define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        \n",
    "        super().__init__()          \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.encoder = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        # try using nn.GRU or nn.RNN here and compare their performances\n",
    "        # try bidirectional and compare their performances\n",
    "        \n",
    "        # Dense layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text = [batch size, sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "    \n",
    "        # Hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs = self.fc(hidden)   \n",
    "        \n",
    "        # Final activation function softmax\n",
    "        output = F.softmax(dense_outputs[0], dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "size_of_vocab = len(ReviewComment.vocab)\n",
    "embedding_dim = 50\n",
    "num_hidden_nodes = 100\n",
    "num_output_nodes = 5\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(16523, 50)\n",
      "  (encoder): LSTM(50, 100, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "The model has 968,255 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    _, predictions = torch.max(preds, 1)\n",
    "    \n",
    "    correct = (predictions == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "# push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    # initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        # retrieve text and no. of words\n",
    "        review, review_lengths = batch.review   \n",
    "        \n",
    "        # convert to 1D tensor\n",
    "        predictions = model(review, review_lengths).squeeze()\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.rating)        \n",
    "        \n",
    "        # compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.rating)   \n",
    "        \n",
    "        # backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        # loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            # retrieve text and no. of words\n",
    "            \n",
    "            review, review_lengths = batch.review   \n",
    "            # convert to 1D tensor\n",
    "            predictions = model(review, review_lengths).squeeze()  \n",
    "            \n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.rating)\n",
    "            acc = binary_accuracy(predictions, batch.rating)\n",
    "            \n",
    "            # keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tTrain Loss: 1.587 | Train Acc: 27.31%\n",
      "\t Val. Loss: 1.580 |  Val. Acc: 26.48% \n",
      "\n",
      "\tTrain Loss: 1.571 | Train Acc: 29.02%\n",
      "\t Val. Loss: 1.576 |  Val. Acc: 28.98% \n",
      "\n",
      "\tTrain Loss: 1.562 | Train Acc: 31.48%\n",
      "\t Val. Loss: 1.571 |  Val. Acc: 29.73% \n",
      "\n",
      "\tTrain Loss: 1.551 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.572 |  Val. Acc: 30.63% \n",
      "\n",
      "\tTrain Loss: 1.539 | Train Acc: 34.46%\n",
      "\t Val. Loss: 1.567 |  Val. Acc: 30.14% \n",
      "\n",
      "\tTrain Loss: 1.527 | Train Acc: 35.93%\n",
      "\t Val. Loss: 1.561 |  Val. Acc: 31.49% \n",
      "\n",
      "\tTrain Loss: 1.516 | Train Acc: 37.28%\n",
      "\t Val. Loss: 1.558 |  Val. Acc: 32.15% \n",
      "\n",
      "\tTrain Loss: 1.504 | Train Acc: 38.44%\n",
      "\t Val. Loss: 1.553 |  Val. Acc: 32.19% \n",
      "\n",
      "\tTrain Loss: 1.489 | Train Acc: 39.96%\n",
      "\t Val. Loss: 1.553 |  Val. Acc: 32.08% \n",
      "\n",
      "\tTrain Loss: 1.476 | Train Acc: 41.42%\n",
      "\t Val. Loss: 1.548 |  Val. Acc: 32.75% \n",
      "\n",
      "\tTrain Loss: 1.462 | Train Acc: 43.60%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 31.52% \n",
      "\n",
      "\tTrain Loss: 1.449 | Train Acc: 45.05%\n",
      "\t Val. Loss: 1.550 |  Val. Acc: 31.71% \n",
      "\n",
      "\tTrain Loss: 1.439 | Train Acc: 46.31%\n",
      "\t Val. Loss: 1.546 |  Val. Acc: 32.64% \n",
      "\n",
      "\tTrain Loss: 1.424 | Train Acc: 48.15%\n",
      "\t Val. Loss: 1.549 |  Val. Acc: 32.31% \n",
      "\n",
      "\tTrain Loss: 1.410 | Train Acc: 49.60%\n",
      "\t Val. Loss: 1.549 |  Val. Acc: 33.10% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    # evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights and tokenizer\n",
    "\n",
    "path='./saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
    "tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "#inference \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def classify_review_comment(review):\n",
    "    \n",
    "    categories = {0: \"very negative\", 1:  \"negative\", 2 : \"neutral\", 3: \"positive\", 4: \"very positive\"}\n",
    "    \n",
    "    # tokenize the tweet \n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
    "    # convert to integer sequence using predefined tokenizer dictionary\n",
    "    indexed = [tokenizer[t] for t in tokenized]        \n",
    "    # compute no. of words        \n",
    "    length = [len(indexed)]\n",
    "    # convert to tensor                                    \n",
    "    tensor = torch.LongTensor(indexed).to(device)   \n",
    "    # reshape in form of batch, no. of words           \n",
    "    tensor = tensor.unsqueeze(1).T  \n",
    "    # convert to tensor                          \n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    # Get the model prediction                  \n",
    "    prediction = model(tensor, length_tensor)\n",
    "\n",
    "    _, pred = torch.max(prediction, 1) \n",
    "    \n",
    "    return categories[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "classify_review_comment(\" !!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translating to latin\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[ \" You are one of many one by one \\', \\' two by two in pairs at the 4 \\'] '"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "from utils.augumentations import *\n",
    "back_translation([\"One by one we'll make many one\", \"Two by two is equals to 4\"])\n"
   ]
  }
 ]
}