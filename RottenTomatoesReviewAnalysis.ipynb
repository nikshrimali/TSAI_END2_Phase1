{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0fbeaffcba064d35a6d29d75e61155a23ece1c55ca7e7176fe096951437ca84b8",
   "display_name": "Python 3.8.5 64-bit ('nlp_py': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c00067e8b0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import torch, torchtext\n",
    "from torchtext.legacy import data\n",
    "import os, pickle\n",
    "from utils.augumentations import *\n",
    "# Manual Seed\n",
    "SEED = 43\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "source": [
    "## Dataset creation\n",
    "The `Stanford Sentiment Treebank (SST)` dataset was presented in “Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank” research from stanford in 2013. While I was not able to extract the complete sentences from this dataset, as it was (as the name suggests) in a tree structure. What I did was I took the phrases and made the dataset out of it. \n",
    "Read more about SST [here](https://towardsdatascience.com/the-stanford-sentiment-treebank-sst-studying-sentiment-analysis-using-nlp-e1a4cad03065)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/stanfordSentimentTreebank/datasetSentences.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-602772dccd07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/stanfordSentimentTreebank/datasetSentences.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"    \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/stanfordSentimentTreebank/datasetSplit.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nlp_py\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/stanfordSentimentTreebank/datasetSentences.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_csv(\"data/stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\t\")\n",
    "\n",
    "dataset_split = pd.read_csv(\"data/stanfordSentimentTreebank/datasetSplit.txt\")\n",
    "split_data = sentences.merge(dataset_split, on=\"sentence_index\", how=\"left\")\n",
    "\n",
    "split_data.columns = [\"sentence_index\", \"phrase\", \"splitset_label\"]\n",
    "\n",
    "split_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          phrase  phrase ids  sentiment values overall sentiment  \\\n",
       "0            ! '       22935           0.52778           neutral   \n",
       "1           ! ''       18235           0.50000           neutral   \n",
       "2         ! Alas      179257           0.44444           neutral   \n",
       "3    ! Brilliant       22936           0.86111     very positive   \n",
       "4  ! Brilliant !       40532           0.93056     very positive   \n",
       "\n",
       "  sentiment_value  \n",
       "0               2  \n",
       "1               2  \n",
       "2               2  \n",
       "3               4  \n",
       "4               4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>phrase ids</th>\n      <th>sentiment values</th>\n      <th>overall sentiment</th>\n      <th>sentiment_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>! '</td>\n      <td>22935</td>\n      <td>0.52778</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>! ''</td>\n      <td>18235</td>\n      <td>0.50000</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>! Alas</td>\n      <td>179257</td>\n      <td>0.44444</td>\n      <td>neutral</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>! Brilliant</td>\n      <td>22936</td>\n      <td>0.86111</td>\n      <td>very positive</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>! Brilliant !</td>\n      <td>40532</td>\n      <td>0.93056</td>\n      <td>very positive</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dictionary = pd.read_csv(\"data/stanfordSentimentTreebank/dictionary.txt\", sep=\"|\")\n",
    "dictionary.columns = [\"phrase\", \"phrase ids\"]\n",
    "# dictionary = dictionary.set_index(\"phrase ids\")\n",
    "\n",
    "sentiment_labels = pd.read_csv(\"data/stanfordSentimentTreebank/sentiment_labels.txt\", sep=\"|\")\n",
    "\n",
    "sentiment_labels[\"overall sentiment\"] = pd.cut(sentiment_labels[\"sentiment values\"], [0, 0.2, 0.4, 0.6, 0.8, 1.0], include_lowest=True ,labels=[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"])\n",
    "\n",
    "sentiment_labels[\"sentiment_value\"] = pd.cut(sentiment_labels[\"sentiment values\"], [0, 0.2, 0.4, 0.6, 0.8, 1.0], include_lowest=True ,labels=[0, 1, 2, 3, 4]) # categorizing the sentiments from 0 to 4, 0 being very negative, 4 being very positive\n",
    "\n",
    "phrase_sentiments = dictionary.merge(sentiment_labels, on=\"phrase ids\")\n",
    "\n",
    "phrase_sentiments.head(5)"
   ]
  },
  {
   "source": [
    "### Cleaning the dataset\n",
    "The phrase are not necessarily in correct english sentences, hence it is critical to clean this data. I removed extra characters at the begining of the phrase, only allowed phrases with length greater than 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                phrase  phrase ids  \\\n",
       "0    The Sum of All Fears is simply a well-made and...      102340   \n",
       "1                              They 're out there ! ''      221244   \n",
       "2    is a temporal inquiry that shoulders its philo...      221388   \n",
       "3           I also wanted a little alien as a friend !      221714   \n",
       "4    West Coast rap wars , this modern mob music dr...      221716   \n",
       "5                      greaseballs mob action-comedy .      142685   \n",
       "6    spy action flick with Antonio Banderas and Luc...      221720   \n",
       "7    style cross-country adventure ... it has spora...      221722   \n",
       "8                         but certainly hard to hate .      221739   \n",
       "9    but it makes for one of the most purely enjoya...      221741   \n",
       "13                LRB- A -RRB- rare , beautiful film .       13691   \n",
       "14   LRB- Drumline -RRB- is entertaining for what i...       13695   \n",
       "15   LRB- Schweiger is -RRB- talented and terribly ...       13696   \n",
       "16   LRB- Wendigo is -RRB- why we go to the cinema ...       13697   \n",
       "17      Blade II is more enjoyable than the original .       24114   \n",
       "19   Brian De Palma is utterly mad : cinema mad , s...      221765   \n",
       "20   Designed to provide a mix of smiles and tears ...      221766   \n",
       "22   Hudlin is stuck trying to light a fire with so...      142763   \n",
       "24        In this incarnation its fizz is infectious .      221767   \n",
       "27   Mafia , rap stars and hood rats butt their ugl...       24121   \n",
       "34   Tunney is allowed to build an uncommonly human...       18539   \n",
       "38   a big , baggy , sprawling carnival of a movie ...      142766   \n",
       "41   a boring parade of talking heads and technical...      142768   \n",
       "46   a cute and sometimes side-splittingly funny bl...       24125   \n",
       "51   a fun little timewaster , helped especially by...       43848   \n",
       "54      a good , if not entirely fresh , look at war .       43849   \n",
       "61   a hollow joke told by a cinematic gymnast havi...        3472   \n",
       "66      a magnificent drama well worth tracking down .       43851   \n",
       "88   a story we have n't seen on the big screen bef...       24130   \n",
       "112  an otherwise intense , twist-and-turn thriller...       24136   \n",
       "\n",
       "     sentiment values overall sentiment sentiment_value  sentence_index  \\\n",
       "0             0.88889     very positive               4            4860   \n",
       "1             0.61111          positive               3            7251   \n",
       "2             0.69444          positive               3            5477   \n",
       "3             0.69444          positive               3            5576   \n",
       "4             0.76389          positive               3            2338   \n",
       "5             0.36111          negative               1            7166   \n",
       "6             0.16667     very negative               0           11305   \n",
       "7             0.70833          positive               3           11409   \n",
       "8             0.61111          positive               3            7163   \n",
       "9             0.81944     very positive               4            2732   \n",
       "13            0.95833     very positive               4           11623   \n",
       "14            0.73611          positive               3             224   \n",
       "15            0.77778          positive               3             145   \n",
       "16            0.65278          positive               3              14   \n",
       "17            0.91667     very positive               4             961   \n",
       "19            0.58333           neutral               2            1550   \n",
       "20            0.22222          negative               1            7900   \n",
       "22            0.37500          negative               1            7929   \n",
       "24            0.84722     very positive               4             148   \n",
       "27            0.65278          positive               3            1262   \n",
       "34            0.59722           neutral               2             345   \n",
       "38            0.34722          negative               1            8454   \n",
       "41            0.27778          negative               1            7763   \n",
       "46            0.84722     very positive               4             974   \n",
       "51            0.65278          positive               3            1491   \n",
       "54            0.55556           neutral               2            1479   \n",
       "61            0.26389          negative               1            7575   \n",
       "66            0.86111     very positive               4            1618   \n",
       "88            0.76389          positive               3            1257   \n",
       "112           0.62500          positive               3            1137   \n",
       "\n",
       "     splitset_label  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 1  \n",
       "8                 1  \n",
       "9                 1  \n",
       "13                2  \n",
       "14                2  \n",
       "15                2  \n",
       "16                2  \n",
       "17                2  \n",
       "19                3  \n",
       "20                3  \n",
       "22                2  \n",
       "24                2  \n",
       "27                3  \n",
       "34                2  \n",
       "38                2  \n",
       "41                3  \n",
       "46                2  \n",
       "51                3  \n",
       "54                3  \n",
       "61                3  \n",
       "66                3  \n",
       "88                3  \n",
       "112               3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>phrase ids</th>\n      <th>sentiment values</th>\n      <th>overall sentiment</th>\n      <th>sentiment_value</th>\n      <th>sentence_index</th>\n      <th>splitset_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Sum of All Fears is simply a well-made and...</td>\n      <td>102340</td>\n      <td>0.88889</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>4860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>They 're out there ! ''</td>\n      <td>221244</td>\n      <td>0.61111</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>7251</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>is a temporal inquiry that shoulders its philo...</td>\n      <td>221388</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5477</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I also wanted a little alien as a friend !</td>\n      <td>221714</td>\n      <td>0.69444</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>5576</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>West Coast rap wars , this modern mob music dr...</td>\n      <td>221716</td>\n      <td>0.76389</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>2338</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>greaseballs mob action-comedy .</td>\n      <td>142685</td>\n      <td>0.36111</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7166</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>spy action flick with Antonio Banderas and Luc...</td>\n      <td>221720</td>\n      <td>0.16667</td>\n      <td>very negative</td>\n      <td>0</td>\n      <td>11305</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>style cross-country adventure ... it has spora...</td>\n      <td>221722</td>\n      <td>0.70833</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>11409</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>but certainly hard to hate .</td>\n      <td>221739</td>\n      <td>0.61111</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>7163</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>but it makes for one of the most purely enjoya...</td>\n      <td>221741</td>\n      <td>0.81944</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>2732</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LRB- A -RRB- rare , beautiful film .</td>\n      <td>13691</td>\n      <td>0.95833</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>11623</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>LRB- Drumline -RRB- is entertaining for what i...</td>\n      <td>13695</td>\n      <td>0.73611</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>224</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>LRB- Schweiger is -RRB- talented and terribly ...</td>\n      <td>13696</td>\n      <td>0.77778</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>145</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>LRB- Wendigo is -RRB- why we go to the cinema ...</td>\n      <td>13697</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>14</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Blade II is more enjoyable than the original .</td>\n      <td>24114</td>\n      <td>0.91667</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>961</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Brian De Palma is utterly mad : cinema mad , s...</td>\n      <td>221765</td>\n      <td>0.58333</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>1550</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Designed to provide a mix of smiles and tears ...</td>\n      <td>221766</td>\n      <td>0.22222</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7900</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Hudlin is stuck trying to light a fire with so...</td>\n      <td>142763</td>\n      <td>0.37500</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7929</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>In this incarnation its fizz is infectious .</td>\n      <td>221767</td>\n      <td>0.84722</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>148</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Mafia , rap stars and hood rats butt their ugl...</td>\n      <td>24121</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1262</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Tunney is allowed to build an uncommonly human...</td>\n      <td>18539</td>\n      <td>0.59722</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>345</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>a big , baggy , sprawling carnival of a movie ...</td>\n      <td>142766</td>\n      <td>0.34722</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>8454</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>a boring parade of talking heads and technical...</td>\n      <td>142768</td>\n      <td>0.27778</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7763</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>a cute and sometimes side-splittingly funny bl...</td>\n      <td>24125</td>\n      <td>0.84722</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>974</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>a fun little timewaster , helped especially by...</td>\n      <td>43848</td>\n      <td>0.65278</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1491</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>a good , if not entirely fresh , look at war .</td>\n      <td>43849</td>\n      <td>0.55556</td>\n      <td>neutral</td>\n      <td>2</td>\n      <td>1479</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>a hollow joke told by a cinematic gymnast havi...</td>\n      <td>3472</td>\n      <td>0.26389</td>\n      <td>negative</td>\n      <td>1</td>\n      <td>7575</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>a magnificent drama well worth tracking down .</td>\n      <td>43851</td>\n      <td>0.86111</td>\n      <td>very positive</td>\n      <td>4</td>\n      <td>1618</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>a story we have n't seen on the big screen bef...</td>\n      <td>24130</td>\n      <td>0.76389</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1257</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>an otherwise intense , twist-and-turn thriller...</td>\n      <td>24136</td>\n      <td>0.62500</td>\n      <td>positive</td>\n      <td>3</td>\n      <td>1137</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "r = re.compile(r'.*?([a-zA-Z].*)')\n",
    "\n",
    "main_dataframe = phrase_sentiments.merge(split_data, on=\"phrase\")\n",
    "main_dataframe[\"splitset_label\"] = main_dataframe[\"splitset_label\"].fillna(1).astype(int)\n",
    "main_dataframe = main_dataframe[main_dataframe.phrase.map(len)>4]\n",
    "main_dataframe['phrase'] = main_dataframe['phrase'].apply(lambda row : r.findall(row)[0])\n",
    "\n",
    "main_dataframe = main_dataframe.groupby(\"splitset_label\")\n",
    "main_dataframe.head(10)"
   ]
  },
  {
   "source": [
    "### Categorizing the main dataframe into train, test and validation dataframe\n",
    "and saving them in CSV format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from utils.augumentations import back_translation\n",
    "\n",
    "train_dataframe = main_dataframe.get_group(1)\n",
    "train_dataframe = train_dataframe.set_index(pd.Index(range(train_dataframe.shape[0])))\n",
    "\n",
    "test_dataframe = main_dataframe.get_group(2)\n",
    "test_dataframe = test_dataframe.set_index(pd.Index(range(test_dataframe.shape[0])))\n",
    "\n",
    "validaion_dataframe = main_dataframe.get_group(3)\n",
    "validaion_dataframe = validaion_dataframe.set_index(pd.Index(range(validaion_dataframe.shape[0])))\n",
    "\n",
    "# train_dataframe.to_csv(\"data/train.csv\")\n",
    "# test_dataframe.to_csv(\"data/test.csv\")\n",
    "# validaion_dataframe.to_csv(\"data/validate.csv\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [
    "## Augmentations\n",
    "Now that we've saved the dataset into our physical memory, we'll apply augmentations (creating data out of data)\n",
    "To do so, we'll be using the following augmentations:\n",
    "* Back Translation - Paraphrasing the sentences, translating a sentence into an another language and converting back into english (source langauge). \n",
    "* Random Swap - Randomly swap words in the sentence\n",
    "* Random Deletion - Randomly delete some of the words\n",
    "\n",
    "Note : I modified the train csv file from google sheets to perform back translation as google translate api limits only 500 translations per day. The main motivation to read data from csv is because of this fact only.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Reading the dataset from csv files (because that's what heroes do)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28230, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "trainset_iterator = train_dataframe.iterrows()\n",
    "\n",
    "for index, row in trainset_iterator:\n",
    "    train_dataframe = train_dataframe.append(\n",
    "            pd.Series(\n",
    "                [\n",
    "                    random_deletion(row['phrase']), row['phrase ids'], \n",
    "                    row['sentiment values'], row['overall sentiment'], \n",
    "                    row['sentiment_value'], row['sentence_index'], \n",
    "                    row['splitset_label']\n",
    "                ],\n",
    "                index = train_dataframe.columns\n",
    "            ),\n",
    "            ignore_index= True\n",
    "    )\n",
    "train_dataframe.to_csv(\"stt_train_with_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv(\"data/stt_train_with_aug.csv\")\n",
    "test_dataframe = pd.read_csv(\"data/test.csv\")\n",
    "validaion_dataframe = pd.read_csv(\"data/validate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReviewComment = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
    "Rating = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('review', ReviewComment),('rating', Rating)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset([data.Example.fromlist([train_dataframe.phrase[i],train_dataframe.sentiment_value[i]], fields) for i in range(train_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.Dataset([data.Example.fromlist([test_dataframe.phrase[i],test_dataframe.sentiment_value[i]], fields) for i in range(test_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'review': ['a',\n",
       "  'haunting',\n",
       "  'vision',\n",
       "  ',',\n",
       "  'with',\n",
       "  'images',\n",
       "  'that',\n",
       "  'seem',\n",
       "  'more',\n",
       "  'like',\n",
       "  'disturbing',\n",
       "  'hallucinations',\n",
       "  '.'],\n",
       " 'rating': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "vars(test_dataset.examples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = data.Dataset([data.Example.fromlist([validaion_dataframe.phrase[i],validaion_dataframe.sentiment_value[i]], fields) for i in range(validaion_dataframe.shape[0])], fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReviewComment.build_vocab(train_dataset)\n",
    "Rating.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of input vocab :  85571\nSize of label vocab :  5\nTop 10 words appreared repeatedly : [('.', 23862), (',', 20692), ('the', 13688), ('a', 11676), ('of', 10520), ('and', 9804), ('to', 6930), ('-', 6751), ('is', 6420), (\"'s\", 5104)]\nLabels :  defaultdict(None, {3: 0, 1: 1, 2: 2, 4: 3, 0: 4})\n"
     ]
    }
   ],
   "source": [
    "print('Size of input vocab : ', len(ReviewComment.vocab))\n",
    "print('Size of label vocab : ', len(Rating.vocab))\n",
    "print('Top 10 words appreared repeatedly :', list(ReviewComment.vocab.freqs.most_common(10)))\n",
    "print('Labels : ', Rating.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = data.BucketIterator.splits((train_dataset, test_dataset), batch_size = 32, \n",
    "                                                            sort_key = lambda x: len(x.review),\n",
    "                                                            sort_within_batch=True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as tokens: \n",
    "    pickle.dump(ReviewComment.vocab.stoi, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    # Define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        \n",
    "        super().__init__()          \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.encoder = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        # try using nn.GRU or nn.RNN here and compare their performances\n",
    "        # try bidirectional and compare their performances\n",
    "        \n",
    "        # Dense layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text = [batch size, sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "    \n",
    "        # Hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs = self.fc(hidden)   \n",
    "        \n",
    "        # Final activation function softmax\n",
    "        output = F.softmax(dense_outputs[0], dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "size_of_vocab = len(ReviewComment.vocab)\n",
    "embedding_dim = 50\n",
    "num_hidden_nodes = 100\n",
    "num_output_nodes = 5\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "classifier(\n  (embedding): Embedding(85571, 50)\n  (encoder): LSTM(50, 100, num_layers=2, batch_first=True, dropout=0.5)\n  (fc): Linear(in_features=100, out_features=5, bias=True)\n)\nThe model has 4,420,655 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    _, predictions = torch.max(preds, 1)\n",
    "    \n",
    "    correct = (predictions == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "# push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    # initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        # retrieve text and no. of words\n",
    "        review, review_lengths = batch.review   \n",
    "        \n",
    "        # convert to 1D tensor\n",
    "        predictions = model(review, review_lengths).squeeze()\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.rating)        \n",
    "        \n",
    "        # compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.rating)   \n",
    "        \n",
    "        # backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        # loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            # retrieve text and no. of words\n",
    "            \n",
    "            review, review_lengths = batch.review   \n",
    "            # convert to 1D tensor\n",
    "            predictions = model(review, review_lengths).squeeze()  \n",
    "            \n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.rating)\n",
    "            acc = binary_accuracy(predictions, batch.rating)\n",
    "            \n",
    "            # keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tTrain Loss: 1.577 | Train Acc: 27.62%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 23.64% \n",
      "\n",
      "\tTrain Loss: 1.568 | Train Acc: 29.31%\n",
      "\t Val. Loss: 1.575 |  Val. Acc: 28.26% \n",
      "\n",
      "\tTrain Loss: 1.553 | Train Acc: 32.21%\n",
      "\t Val. Loss: 1.561 |  Val. Acc: 31.89% \n",
      "\n",
      "\tTrain Loss: 1.534 | Train Acc: 35.07%\n",
      "\t Val. Loss: 1.555 |  Val. Acc: 32.50% \n",
      "\n",
      "\tTrain Loss: 1.514 | Train Acc: 37.28%\n",
      "\t Val. Loss: 1.561 |  Val. Acc: 31.89% \n",
      "\n",
      "\tTrain Loss: 1.496 | Train Acc: 39.44%\n",
      "\t Val. Loss: 1.551 |  Val. Acc: 33.10% \n",
      "\n",
      "\tTrain Loss: 1.477 | Train Acc: 41.58%\n",
      "\t Val. Loss: 1.546 |  Val. Acc: 34.48% \n",
      "\n",
      "\tTrain Loss: 1.458 | Train Acc: 43.91%\n",
      "\t Val. Loss: 1.547 |  Val. Acc: 34.01% \n",
      "\n",
      "\tTrain Loss: 1.439 | Train Acc: 45.96%\n",
      "\t Val. Loss: 1.542 |  Val. Acc: 34.64% \n",
      "\n",
      "\tTrain Loss: 1.420 | Train Acc: 48.16%\n",
      "\t Val. Loss: 1.556 |  Val. Acc: 33.15% \n",
      "\n",
      "\tTrain Loss: 1.403 | Train Acc: 49.70%\n",
      "\t Val. Loss: 1.548 |  Val. Acc: 34.20% \n",
      "\n",
      "\tTrain Loss: 1.384 | Train Acc: 51.99%\n",
      "\t Val. Loss: 1.547 |  Val. Acc: 34.36% \n",
      "\n",
      "\tTrain Loss: 1.369 | Train Acc: 53.59%\n",
      "\t Val. Loss: 1.557 |  Val. Acc: 32.36% \n",
      "\n",
      "\tTrain Loss: 1.353 | Train Acc: 55.34%\n",
      "\t Val. Loss: 1.549 |  Val. Acc: 34.31% \n",
      "\n",
      "\tTrain Loss: 1.339 | Train Acc: 56.67%\n",
      "\t Val. Loss: 1.565 |  Val. Acc: 31.89% \n",
      "\n",
      "\tTrain Loss: 1.325 | Train Acc: 58.16%\n",
      "\t Val. Loss: 1.560 |  Val. Acc: 33.03% \n",
      "\n",
      "\tTrain Loss: 1.312 | Train Acc: 59.40%\n",
      "\t Val. Loss: 1.563 |  Val. Acc: 33.01% \n",
      "\n",
      "\tTrain Loss: 1.302 | Train Acc: 60.40%\n",
      "\t Val. Loss: 1.557 |  Val. Acc: 33.45% \n",
      "\n",
      "\tTrain Loss: 1.293 | Train Acc: 61.35%\n",
      "\t Val. Loss: 1.562 |  Val. Acc: 33.43% \n",
      "\n",
      "\tTrain Loss: 1.285 | Train Acc: 62.21%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 33.27% \n",
      "\n",
      "\tTrain Loss: 1.275 | Train Acc: 63.02%\n",
      "\t Val. Loss: 1.554 |  Val. Acc: 33.92% \n",
      "\n",
      "\tTrain Loss: 1.270 | Train Acc: 63.56%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 33.32% \n",
      "\n",
      "\tTrain Loss: 1.262 | Train Acc: 64.34%\n",
      "\t Val. Loss: 1.564 |  Val. Acc: 33.01% \n",
      "\n",
      "\tTrain Loss: 1.257 | Train Acc: 64.78%\n",
      "\t Val. Loss: 1.575 |  Val. Acc: 31.87% \n",
      "\n",
      "\tTrain Loss: 1.253 | Train Acc: 65.20%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 33.92% \n",
      "\n",
      "\tTrain Loss: 1.245 | Train Acc: 65.97%\n",
      "\t Val. Loss: 1.568 |  Val. Acc: 32.52% \n",
      "\n",
      "\tTrain Loss: 1.238 | Train Acc: 66.64%\n",
      "\t Val. Loss: 1.562 |  Val. Acc: 33.03% \n",
      "\n",
      "\tTrain Loss: 1.235 | Train Acc: 67.11%\n",
      "\t Val. Loss: 1.568 |  Val. Acc: 32.50% \n",
      "\n",
      "\tTrain Loss: 1.229 | Train Acc: 67.59%\n",
      "\t Val. Loss: 1.570 |  Val. Acc: 32.01% \n",
      "\n",
      "\tTrain Loss: 1.223 | Train Acc: 68.14%\n",
      "\t Val. Loss: 1.568 |  Val. Acc: 32.24% \n",
      "\n",
      "\tTrain Loss: 1.216 | Train Acc: 68.93%\n",
      "\t Val. Loss: 1.561 |  Val. Acc: 33.55% \n",
      "\n",
      "\tTrain Loss: 1.210 | Train Acc: 69.50%\n",
      "\t Val. Loss: 1.566 |  Val. Acc: 32.64% \n",
      "\n",
      "\tTrain Loss: 1.203 | Train Acc: 70.14%\n",
      "\t Val. Loss: 1.564 |  Val. Acc: 32.96% \n",
      "\n",
      "\tTrain Loss: 1.200 | Train Acc: 70.54%\n",
      "\t Val. Loss: 1.571 |  Val. Acc: 32.31% \n",
      "\n",
      "\tTrain Loss: 1.190 | Train Acc: 71.48%\n",
      "\t Val. Loss: 1.567 |  Val. Acc: 33.08% \n",
      "\n",
      "\tTrain Loss: 1.185 | Train Acc: 72.08%\n",
      "\t Val. Loss: 1.569 |  Val. Acc: 32.83% \n",
      "\n",
      "\tTrain Loss: 1.180 | Train Acc: 72.48%\n",
      "\t Val. Loss: 1.572 |  Val. Acc: 31.96% \n",
      "\n",
      "\tTrain Loss: 1.175 | Train Acc: 72.95%\n",
      "\t Val. Loss: 1.578 |  Val. Acc: 31.41% \n",
      "\n",
      "\tTrain Loss: 1.171 | Train Acc: 73.30%\n",
      "\t Val. Loss: 1.569 |  Val. Acc: 32.45% \n",
      "\n",
      "\tTrain Loss: 1.168 | Train Acc: 73.78%\n",
      "\t Val. Loss: 1.575 |  Val. Acc: 31.50% \n",
      "\n",
      "\tTrain Loss: 1.164 | Train Acc: 74.14%\n",
      "\t Val. Loss: 1.569 |  Val. Acc: 32.20% \n",
      "\n",
      "\tTrain Loss: 1.159 | Train Acc: 74.58%\n",
      "\t Val. Loss: 1.577 |  Val. Acc: 31.59% \n",
      "\n",
      "\tTrain Loss: 1.156 | Train Acc: 74.92%\n",
      "\t Val. Loss: 1.576 |  Val. Acc: 31.45% \n",
      "\n",
      "\tTrain Loss: 1.153 | Train Acc: 75.17%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 30.54% \n",
      "\n",
      "\tTrain Loss: 1.150 | Train Acc: 75.39%\n",
      "\t Val. Loss: 1.579 |  Val. Acc: 31.31% \n",
      "\n",
      "\tTrain Loss: 1.149 | Train Acc: 75.61%\n",
      "\t Val. Loss: 1.581 |  Val. Acc: 31.03% \n",
      "\n",
      "\tTrain Loss: 1.146 | Train Acc: 75.83%\n",
      "\t Val. Loss: 1.577 |  Val. Acc: 31.45% \n",
      "\n",
      "\tTrain Loss: 1.144 | Train Acc: 76.03%\n",
      "\t Val. Loss: 1.573 |  Val. Acc: 32.10% \n",
      "\n",
      "\tTrain Loss: 1.141 | Train Acc: 76.25%\n",
      "\t Val. Loss: 1.579 |  Val. Acc: 31.43% \n",
      "\n",
      "\tTrain Loss: 1.139 | Train Acc: 76.42%\n",
      "\t Val. Loss: 1.579 |  Val. Acc: 31.43% \n",
      "\n",
      "\tTrain Loss: 1.138 | Train Acc: 76.55%\n",
      "\t Val. Loss: 1.583 |  Val. Acc: 31.08% \n",
      "\n",
      "\tTrain Loss: 1.134 | Train Acc: 76.99%\n",
      "\t Val. Loss: 1.578 |  Val. Acc: 31.45% \n",
      "\n",
      "\tTrain Loss: 1.132 | Train Acc: 77.14%\n",
      "\t Val. Loss: 1.583 |  Val. Acc: 30.85% \n",
      "\n",
      "\tTrain Loss: 1.132 | Train Acc: 77.12%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 31.03% \n",
      "\n",
      "\tTrain Loss: 1.129 | Train Acc: 77.42%\n",
      "\t Val. Loss: 1.587 |  Val. Acc: 30.66% \n",
      "\n",
      "\tTrain Loss: 1.126 | Train Acc: 77.68%\n",
      "\t Val. Loss: 1.585 |  Val. Acc: 30.89% \n",
      "\n",
      "\tTrain Loss: 1.124 | Train Acc: 77.90%\n",
      "\t Val. Loss: 1.586 |  Val. Acc: 30.80% \n",
      "\n",
      "\tTrain Loss: 1.122 | Train Acc: 78.21%\n",
      "\t Val. Loss: 1.578 |  Val. Acc: 31.57% \n",
      "\n",
      "\tTrain Loss: 1.120 | Train Acc: 78.36%\n",
      "\t Val. Loss: 1.590 |  Val. Acc: 30.24% \n",
      "\n",
      "\tTrain Loss: 1.115 | Train Acc: 78.96%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 30.57% \n",
      "\n",
      "\tTrain Loss: 1.109 | Train Acc: 79.60%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.75% \n",
      "\n",
      "\tTrain Loss: 1.102 | Train Acc: 80.32%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 30.73% \n",
      "\n",
      "\tTrain Loss: 1.096 | Train Acc: 81.00%\n",
      "\t Val. Loss: 1.581 |  Val. Acc: 31.27% \n",
      "\n",
      "\tTrain Loss: 1.091 | Train Acc: 81.52%\n",
      "\t Val. Loss: 1.586 |  Val. Acc: 30.55% \n",
      "\n",
      "\tTrain Loss: 1.087 | Train Acc: 81.95%\n",
      "\t Val. Loss: 1.586 |  Val. Acc: 30.85% \n",
      "\n",
      "\tTrain Loss: 1.081 | Train Acc: 82.46%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.94% \n",
      "\n",
      "\tTrain Loss: 1.077 | Train Acc: 82.90%\n",
      "\t Val. Loss: 1.588 |  Val. Acc: 30.52% \n",
      "\n",
      "\tTrain Loss: 1.073 | Train Acc: 83.29%\n",
      "\t Val. Loss: 1.590 |  Val. Acc: 30.22% \n",
      "\n",
      "\tTrain Loss: 1.072 | Train Acc: 83.39%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.84% \n",
      "\n",
      "\tTrain Loss: 1.069 | Train Acc: 83.60%\n",
      "\t Val. Loss: 1.594 |  Val. Acc: 30.17% \n",
      "\n",
      "\tTrain Loss: 1.065 | Train Acc: 83.99%\n",
      "\t Val. Loss: 1.594 |  Val. Acc: 29.80% \n",
      "\n",
      "\tTrain Loss: 1.064 | Train Acc: 84.13%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 30.01% \n",
      "\n",
      "\tTrain Loss: 1.061 | Train Acc: 84.32%\n",
      "\t Val. Loss: 1.589 |  Val. Acc: 30.40% \n",
      "\n",
      "\tTrain Loss: 1.060 | Train Acc: 84.44%\n",
      "\t Val. Loss: 1.593 |  Val. Acc: 29.98% \n",
      "\n",
      "\tTrain Loss: 1.058 | Train Acc: 84.67%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.89% \n",
      "\n",
      "\tTrain Loss: 1.056 | Train Acc: 84.80%\n",
      "\t Val. Loss: 1.598 |  Val. Acc: 29.56% \n",
      "\n",
      "\tTrain Loss: 1.057 | Train Acc: 84.70%\n",
      "\t Val. Loss: 1.596 |  Val. Acc: 29.54% \n",
      "\n",
      "\tTrain Loss: 1.054 | Train Acc: 84.94%\n",
      "\t Val. Loss: 1.600 |  Val. Acc: 29.24% \n",
      "\n",
      "\tTrain Loss: 1.053 | Train Acc: 85.14%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.82% \n",
      "\n",
      "\tTrain Loss: 1.051 | Train Acc: 85.22%\n",
      "\t Val. Loss: 1.596 |  Val. Acc: 30.10% \n",
      "\n",
      "\tTrain Loss: 1.051 | Train Acc: 85.28%\n",
      "\t Val. Loss: 1.600 |  Val. Acc: 29.52% \n",
      "\n",
      "\tTrain Loss: 1.050 | Train Acc: 85.34%\n",
      "\t Val. Loss: 1.596 |  Val. Acc: 29.78% \n",
      "\n",
      "\tTrain Loss: 1.048 | Train Acc: 85.51%\n",
      "\t Val. Loss: 1.600 |  Val. Acc: 29.24% \n",
      "\n",
      "\tTrain Loss: 1.047 | Train Acc: 85.61%\n",
      "\t Val. Loss: 1.593 |  Val. Acc: 30.17% \n",
      "\n",
      "\tTrain Loss: 1.048 | Train Acc: 85.53%\n",
      "\t Val. Loss: 1.595 |  Val. Acc: 30.48% \n",
      "\n",
      "\tTrain Loss: 1.047 | Train Acc: 85.60%\n",
      "\t Val. Loss: 1.595 |  Val. Acc: 29.92% \n",
      "\n",
      "\tTrain Loss: 1.045 | Train Acc: 85.80%\n",
      "\t Val. Loss: 1.601 |  Val. Acc: 29.42% \n",
      "\n",
      "\tTrain Loss: 1.044 | Train Acc: 85.81%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.89% \n",
      "\n",
      "\tTrain Loss: 1.043 | Train Acc: 85.93%\n",
      "\t Val. Loss: 1.602 |  Val. Acc: 29.45% \n",
      "\n",
      "\tTrain Loss: 1.045 | Train Acc: 85.83%\n",
      "\t Val. Loss: 1.603 |  Val. Acc: 29.12% \n",
      "\n",
      "\tTrain Loss: 1.042 | Train Acc: 86.05%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 31.22% \n",
      "\n",
      "\tTrain Loss: 1.043 | Train Acc: 85.98%\n",
      "\t Val. Loss: 1.595 |  Val. Acc: 29.92% \n",
      "\n",
      "\tTrain Loss: 1.042 | Train Acc: 86.04%\n",
      "\t Val. Loss: 1.599 |  Val. Acc: 29.63% \n",
      "\n",
      "\tTrain Loss: 1.042 | Train Acc: 86.10%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.75% \n",
      "\n",
      "\tTrain Loss: 1.041 | Train Acc: 86.16%\n",
      "\t Val. Loss: 1.594 |  Val. Acc: 30.24% \n",
      "\n",
      "\tTrain Loss: 1.039 | Train Acc: 86.29%\n",
      "\t Val. Loss: 1.600 |  Val. Acc: 29.75% \n",
      "\n",
      "\tTrain Loss: 1.038 | Train Acc: 86.37%\n",
      "\t Val. Loss: 1.598 |  Val. Acc: 29.70% \n",
      "\n",
      "\tTrain Loss: 1.039 | Train Acc: 86.36%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 29.54% \n",
      "\n",
      "\tTrain Loss: 1.038 | Train Acc: 86.39%\n",
      "\t Val. Loss: 1.592 |  Val. Acc: 30.03% \n",
      "\n",
      "\tTrain Loss: 1.038 | Train Acc: 86.38%\n",
      "\t Val. Loss: 1.602 |  Val. Acc: 29.28% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    # evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights and tokenizer\n",
    "\n",
    "path='./saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
    "tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "#inference \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def classify_review_comment(review):\n",
    "    \n",
    "    categories = {0: \"very negative\", 1:  \"negative\", 2 : \"neutral\", 3: \"positive\", 4: \"very positive\"}\n",
    "    \n",
    "    # tokenize the tweet \n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
    "    # convert to integer sequence using predefined tokenizer dictionary\n",
    "    indexed = [tokenizer[t] for t in tokenized]        \n",
    "    # compute no. of words        \n",
    "    length = [len(indexed)]\n",
    "    # convert to tensor                                    \n",
    "    tensor = torch.LongTensor(indexed).to(device)   \n",
    "    # reshape in form of batch, no. of words           \n",
    "    tensor = tensor.unsqueeze(1).T  \n",
    "    # convert to tensor                          \n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    # Get the model prediction                  \n",
    "    prediction = model(tensor, length_tensor)\n",
    "    _, pred = torch.max(prediction, 1) \n",
    "    \n",
    "    return categories[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset_iterator = validaion_dataframe.iterrows()\n",
    "misclassified = pd.DataFrame(columns=['phrase', 'actual rating', 'predicted'])\n",
    "correct_classified = pd.DataFrame(columns=['phrase', 'actual rating', 'predicted'])\n",
    "\n",
    "for index, row in validset_iterator:\n",
    "    predicted_rating = classify_review_comment(row['phrase'])\n",
    "    if predicted_rating != row['overall sentiment']:\n",
    "        misclassified = misclassified.append(\n",
    "            pd.Series([\n",
    "                row['phrase'], row['overall sentiment'], predicted_rating\n",
    "            ], index = misclassified.columns), \n",
    "            ignore_index = True\n",
    "        )\n",
    "    else:\n",
    "        correct_classified = correct_classified.append(\n",
    "            pd.Series([\n",
    "                row['phrase'], row['overall sentiment'], predicted_rating\n",
    "            ], index = misclassified.columns), \n",
    "            ignore_index = True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               phrase  actual rating  \\\n",
       "0   Brian De Palma is utterly mad : cinema mad , s...        neutral   \n",
       "1   Mafia , rap stars and hood rats butt their ugl...       positive   \n",
       "2   a boring parade of talking heads and technical...       negative   \n",
       "3   a fun little timewaster , helped especially by...       positive   \n",
       "4      a good , if not entirely fresh , look at war .        neutral   \n",
       "5      a magnificent drama well worth tracking down .  very positive   \n",
       "6   a story we have n't seen on the big screen bef...       positive   \n",
       "7   an otherwise intense , twist-and-turn thriller...       positive   \n",
       "8   plot holes so large and obvious a marching ban...  very negative   \n",
       "9      routine , harmless diversion and little else .       positive   \n",
       "10            the movie is just a plain old monster .  very negative   \n",
       "11            think of it as American Pie On Valium .        neutral   \n",
       "12  turns so unforgivably trite in its last 10 min...  very negative   \n",
       "13  with `` The Bourne Identity '' we return to th...       positive   \n",
       "14  minutes of rolling musical back beat and super...        neutral   \n",
       "15   A beautifully made piece of unwatchable drivel .        neutral   \n",
       "16  A beguiling , slow-moving parable about the co...        neutral   \n",
       "17  A beguiling splash of pastel colors and pranki...  very positive   \n",
       "18  A big , gorgeous , sprawling swashbuckler that...  very positive   \n",
       "19  A breezy romantic comedy that has the punch of...  very positive   \n",
       "\n",
       "        predicted  \n",
       "0   very negative  \n",
       "1   very negative  \n",
       "2   very negative  \n",
       "3         neutral  \n",
       "4   very negative  \n",
       "5        negative  \n",
       "6   very negative  \n",
       "7         neutral  \n",
       "8        negative  \n",
       "9   very negative  \n",
       "10       negative  \n",
       "11       negative  \n",
       "12       negative  \n",
       "13       negative  \n",
       "14  very negative  \n",
       "15       negative  \n",
       "16       negative  \n",
       "17       negative  \n",
       "18  very negative  \n",
       "19  very negative  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>actual rating</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brian De Palma is utterly mad : cinema mad , s...</td>\n      <td>neutral</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mafia , rap stars and hood rats butt their ugl...</td>\n      <td>positive</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a boring parade of talking heads and technical...</td>\n      <td>negative</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a fun little timewaster , helped especially by...</td>\n      <td>positive</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a good , if not entirely fresh , look at war .</td>\n      <td>neutral</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>a magnificent drama well worth tracking down .</td>\n      <td>very positive</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>a story we have n't seen on the big screen bef...</td>\n      <td>positive</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>an otherwise intense , twist-and-turn thriller...</td>\n      <td>positive</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>plot holes so large and obvious a marching ban...</td>\n      <td>very negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>routine , harmless diversion and little else .</td>\n      <td>positive</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>the movie is just a plain old monster .</td>\n      <td>very negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>think of it as American Pie On Valium .</td>\n      <td>neutral</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>turns so unforgivably trite in its last 10 min...</td>\n      <td>very negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>with `` The Bourne Identity '' we return to th...</td>\n      <td>positive</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minutes of rolling musical back beat and super...</td>\n      <td>neutral</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>A beautifully made piece of unwatchable drivel .</td>\n      <td>neutral</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>A beguiling , slow-moving parable about the co...</td>\n      <td>neutral</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>A beguiling splash of pastel colors and pranki...</td>\n      <td>very positive</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>A big , gorgeous , sprawling swashbuckler that...</td>\n      <td>very positive</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>A breezy romantic comedy that has the punch of...</td>\n      <td>very positive</td>\n      <td>very negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "misclassified.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               phrase  actual rating  \\\n",
       "0   Designed to provide a mix of smiles and tears ...       negative   \n",
       "1   a hollow joke told by a cinematic gymnast havi...       negative   \n",
       "2   is an arthritic attempt at directing by Callie...       negative   \n",
       "3   nothing scary here except for some awful actin...       negative   \n",
       "4   plays like somebody spliced random moments of ...       negative   \n",
       "5   takes the beauty of baseball and melds it with...       positive   \n",
       "6   A better title , for all concerned , might be ...       negative   \n",
       "7   A bloated gasbag thesis grotesquely impressed ...  very negative   \n",
       "8   A broad , melodramatic estrogen opera that 's ...       negative   \n",
       "9   A by-the-numbers effort that wo n't do much to...       negative   \n",
       "10  A dumb movie with dumb characters doing dumb t...       negative   \n",
       "11  A hamfisted romantic comedy that makes our gir...       negative   \n",
       "12  A lackluster , unessential sequel to the class...  very negative   \n",
       "13  A morose little soap opera about three vapid ,...        neutral   \n",
       "14   A sequel that 's much too big for its britches .       negative   \n",
       "15  A sequence of ridiculous shoot - 'em - up scen...       negative   \n",
       "16  A string of rehashed sight gags based in insip...       negative   \n",
       "17  A subject like this should inspire reaction in...       negative   \n",
       "18  A valueless kiddie paean to pro basketball und...       negative   \n",
       "19  Adults will wish the movie were less simplisti...       negative   \n",
       "\n",
       "        predicted  \n",
       "0        negative  \n",
       "1        negative  \n",
       "2        negative  \n",
       "3        negative  \n",
       "4        negative  \n",
       "5        positive  \n",
       "6        negative  \n",
       "7   very negative  \n",
       "8        negative  \n",
       "9        negative  \n",
       "10       negative  \n",
       "11       negative  \n",
       "12  very negative  \n",
       "13        neutral  \n",
       "14       negative  \n",
       "15       negative  \n",
       "16       negative  \n",
       "17       negative  \n",
       "18       negative  \n",
       "19       negative  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>actual rating</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Designed to provide a mix of smiles and tears ...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a hollow joke told by a cinematic gymnast havi...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>is an arthritic attempt at directing by Callie...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nothing scary here except for some awful actin...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>plays like somebody spliced random moments of ...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>takes the beauty of baseball and melds it with...</td>\n      <td>positive</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A better title , for all concerned , might be ...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>A bloated gasbag thesis grotesquely impressed ...</td>\n      <td>very negative</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>A broad , melodramatic estrogen opera that 's ...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A by-the-numbers effort that wo n't do much to...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>A dumb movie with dumb characters doing dumb t...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>A hamfisted romantic comedy that makes our gir...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>A lackluster , unessential sequel to the class...</td>\n      <td>very negative</td>\n      <td>very negative</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>A morose little soap opera about three vapid ,...</td>\n      <td>neutral</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>A sequel that 's much too big for its britches .</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>A sequence of ridiculous shoot - 'em - up scen...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>A string of rehashed sight gags based in insip...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>A subject like this should inspire reaction in...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>A valueless kiddie paean to pro basketball und...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Adults will wish the movie were less simplisti...</td>\n      <td>negative</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "correct_classified.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified.to_csv(\"output/misclassified.csv\")\n",
    "correct_classified.to_csv(\"output/correct_classified.csv\")"
   ]
  }
 ]
}