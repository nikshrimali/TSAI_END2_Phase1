{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nlp_py': conda)"
  },
  "interpreter": {
   "hash": "fbeaffcba064d35a6d29d75e61155a23ece1c55ca7e7176fe096951437ca84b8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\naman\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\naman\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dataset.legacy.tweets_dataset import TweetsDataset\n",
    "dataset = TweetsDataset(transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "train_iterator, valid_iterator = torchtext.legacy.data.BucketIterator.splits((dataset.train_dataset, dataset.validation_dataset), batch_size = 1, sort_key = lambda x: len(x.review), sort_within_batch=True, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(dataset.Tweet.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.lstm import Lstm\n",
    "from model.encoder_decoder import EncoderDecoder\n",
    "encoder = Lstm(vocab_size = size_of_vocab, embedding_dim= embedding_dim, hidden_dim = num_hidden_nodes, staggered_input = True)\n",
    "decoder = Lstm(vocab_size = None, embedding_dim= encoder.fc.out_features, hidden_dim = num_hidden_nodes, output_dim = num_hidden_nodes, staggered_input = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(encoder=encoder, decoder=decoder, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = \"cuda\"\n",
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    _, predictions = torch.max(preds, 1)\n",
    "    \n",
    "    correct = (predictions == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "# push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    # initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        # retrieve text and no. of words\n",
    "        tweet, tweet_lengths = batch.review  \n",
    "        # convert to 1D tensor\n",
    "        predictions = model(tweet, tweet_lengths)\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.rating)        \n",
    "        \n",
    "        # compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.rating)   \n",
    "        \n",
    "        # backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        # loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            # retrieve text and no. of words\n",
    "            tweet, tweet_lengths = batch.review\n",
    "            \n",
    "            # convert to 1d tensor\n",
    "            predictions = model(tweet, tweet_lengths)\n",
    "            \n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.rating)\n",
    "            acc = binary_accuracy(predictions, batch.rating)\n",
    "            \n",
    "            # keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tTrain Loss: 0.824 | Train Acc: 62.99%\n",
      "\t Val. Loss: 0.652 |  Val. Acc: 73.17% \n",
      "\n",
      "\tTrain Loss: 0.627 | Train Acc: 76.10%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 80.49% \n",
      "\n",
      "\tTrain Loss: 0.482 | Train Acc: 84.56%\n",
      "\t Val. Loss: 0.591 |  Val. Acc: 81.46% \n",
      "\n",
      "\tTrain Loss: 0.350 | Train Acc: 88.78%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 76.59% \n",
      "\n",
      "\tTrain Loss: 0.258 | Train Acc: 91.20%\n",
      "\t Val. Loss: 0.747 |  Val. Acc: 73.17% \n",
      "\n",
      "\tTrain Loss: 0.212 | Train Acc: 92.49%\n",
      "\t Val. Loss: 0.823 |  Val. Acc: 74.15% \n",
      "\n",
      "\tTrain Loss: 0.187 | Train Acc: 93.27%\n",
      "\t Val. Loss: 0.891 |  Val. Acc: 72.68% \n",
      "\n",
      "\tTrain Loss: 0.157 | Train Acc: 93.70%\n",
      "\t Val. Loss: 0.911 |  Val. Acc: 74.63% \n",
      "\n",
      "\tTrain Loss: 0.135 | Train Acc: 93.87%\n",
      "\t Val. Loss: 0.943 |  Val. Acc: 73.66% \n",
      "\n",
      "\tTrain Loss: 0.090 | Train Acc: 95.69%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 77.56% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    # evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
   ]
  }
 ]
}